# Human-Pose-Recognition-in-Videogames

## Descrizione
Questo progetto consiste, sulla base di un sistema per il rilevamento facciale gi√† sviluppato da terzi, nella stima della direzione del volto e la sua applicazione in ambientazioni 3D e videogames.
L'obiettivo √® stato quello di creare un sistema in grado di acquisire ed elaborare una sequenza di immagini allo scopo di estrarre le informazioni relative al volto rilevato per inviarle ad un server tcp, incaricato di decodificarle per creare una mappatura 3D e infine determinare il suo orientamento.

![Descrizione ](images/image.png)

Il progetto si articola in tre fasi essenziali: 

‚Ä¢ Acquisizione delle immagini per il rilevamento del volto

‚Ä¢ Estrazione delle informazioni del volto rilevato e mappatura in 3D

‚Ä¢ Identificazione dell‚Äôorientamento del volto

La prima fase comprende l‚Äôacquisizione di una sequenza di immagini da webcam e il rilevamento del volto. L‚Äôacquisizione delle immagini √® ottenuta grazie a un algoritmo sviluppato in python che sfrutta la libreria OpenCV e Scikit-image. Il sistema impiega una rete neurale in grado di localizzare 68 punti di riferimento del viso (landmarks). I landmarks vengono organizzati in 9 aree specifiche del viso: contorno facciale, sopracciglio sinistro e destro, naso, narici, occhio sinistro e destro, labbra e denti. La seconda fase comprende l‚Äôestrazione delle informazioni, la codifica in un formato specifico, il loro invio a un server tcp in Unity e la mappatura 3D. Le informazioni ottenute riguardano le coordinate di ciascun landmark nelle posizioni x, y e z. Tali informazioni vengono manipolate e inviate al server tcp, permettendo la creazione di una mesh 3D del viso. La terza fase comprende la stima della direzione del volto, ottenuta sfruttando 3 landmarks specifici dai quali √® possibile calcolare il punto esatto in cui l‚Äôutente sta guardando, grazie alla creazione di un raggio luminoso. Infine tale lavoro √® stato integrato in diversi ambienti di gioco, mettendone in evidenza il suo scopo finale.

## Ambiente di sviluppo  üíª 

Per l'ambiente di sviluppo e relativa gestione delle librerie √® stato utilizzato Anaconda Navigator, da cui sono state installate e sfruttate le seguenti librerie Open source:

‚Ä¢ OpenCV

‚Ä¢ Scikit-image

‚Ä¢ Matplotlib

La libreria OpenCV √® stata principalmente utilizzata per l‚Äôapertura della fotocamera, l‚Äôacquisizione delle immagini e la rilevazione del volto. La libreria Skimage √® stata utilizzata per l‚Äôelaborazione delle immagini acquisite, mentre la libreria Matplotlib √® stata sfruttata per la visualizzazione dei grafici 2D e 3D generati dall‚Äôimmagine e per ottenere le informazioni fondamentali per la mappatura 3D del volto.

## Acquisizione delle immagini üì∏

![Acquisizione delle immagini ](images/face-scan.png)

Il primo passo per lo sviluppo di tale sistema √® stata l‚Äôacquisizione di una sequenza di immagini. Inizialmente si √® partito da un‚Äôanalisi approfondita del codice, sviluppato da Adrian Bulat, ricercatore di intelligenza artificiale presso l‚ÄôUniversit√† di Nottingham, il quale ha realizzato un sistema per l‚Äôaddestramento della rete FAN applicata a esperimenti di addestramento e allineamento facciale 2D e 3D. Costruita sulla base di un‚Äôavanzata architettura per la stima della posa, chiamata **HourGlass** (HG), la rete prende in input l‚Äôimmagine RGB, ottiene i punti di riferimento 2D e genera le corrispondenti proiezioni 2D e 3D, passandole successivamente a una seconda rete, responsabile del riconoscimento vero e proprio, chiamata **Facial Recognition Network** (FRN). 

```sh
cam = cv2.VideoCapture(0, cv2.CAP_DSHOW)
count = 0
ListImage = []
while True:
  ret, img = cam.read()
  cv2.imshow("ImageCapture", img)
  if not ret:
    break
  k=cv2.waitKey(2)
  if k%256 == 27:
    break
  else:
    print("Image "+str(count)+" saved...")
    path='../test/assets/'+str(count)+'.jpg'
    cv2.imwrite(path, img)
    ListImage.append(path)
    time.sleep(1)
    count +=1
cam.release()
cv2.destroyAllWindows()
```

Come possibile vedere in **detect_landmarks_in_real-time.py** e dal precedente snippet, l‚Äôidea si basa su un‚Äôacquisizione di pi√π immagini simultaneamente, principalmente per poter garantire una rappresentazione real-time del movimento della mappatura 3D che si andr√† a creare. √à stato quindi implementato un semplice algoritmo che tramite l‚Äôutilizzo della libreria OpenCV, viene aperto il frame relativo alla fotocamera per l‚Äôacquisizione delle immagini con un intervallo di tempo per ognuna impostato ad un secondo, principalmente per permettere all‚Äôutente di poter uscire in qualsiasi momento dalla procedura. Nel momento in cui si ritiene conclusa la procedura, √® stata aggiunta la possibilit√† di fare in modo che cliccando sul tasto ‚ÄúEsc‚Äù, l‚Äôutente concluda l‚Äôoperazione di acquisizione. L‚Äôimmagine o sequenza di immagini acquisite vengono salvate in un path specifico e i nomi delle immagini sono inserite in un array, incrementando di volta in volta una variabile contatore. 

Vediamo ora in dettaglio il seguente codice:

```sh
l = 0
while l<len(ListImage):
  try:
    print("Avvio Riconoscimento Volto n."+str(l))
    input_img = io.imread(ListImage[l])
  except FileNotFoundError:
    input_img = io.imread('../test/assets/aflw-test.jpg')
```

La variabile contatore viene utilizzata per scorrere l‚Äôarray, in modo tale da passare singolarmente ciascun path alla rete FAN, la quale proceder√† al rilevamento e all‚Äôallineamento facciale. Nel caso in cui la rete non rilevi la presenza di un volto, √® stata data come eccezione un‚Äôimmagine di test, mostrando all‚Äôutente da terminale un messaggio di mancato riconoscimento facciale.

## Rilevamento e Riconoscimento del volto

Una volta acquisite le immagini si √® proceduto al rilevamento e riconoscimento del volto, ottenuto sfruttando la rete FAN (Face-Alignment Network), in grado di ricavare le proiezioni 2D e 3D dei punti di riferimento facciali.

### Face-Alignment Network

![Face-Alignment Network ](images/fan.png)

In tale sistema √® stata impiegata la ‚ÄúFace-Alignment Network‚Äù (FAN). Sviluppata da Adrian Bulat, FAN √® una rete neurale all‚Äôavanguardia per la localizzazione dei punti di riferimento del volto, pre-addestrata per l'allineamento di volti in 2D e 3D e valutata su centinaia di migliaia di immagini. Tale rete si √® mostrata fin da subito affidabile e con un basso margine di errori, garantendo una considerevole capacit√† di resistenza a posa, risoluzione e illuminazione. I test eseguiti da Adrian Bulat su set di dati indipendenti, rispettivamente per il 2D, Dal 2D al 3D e il 3D, hanno mostrato ottimi risultati. Lo sviluppo del progetto non ha previsto un addestramento della macchina utilizzata, poich√© la rete risulta pre-addestrata e quindi in grado di poter effettuare senza alcun problema il riconoscimento e l‚Äôallineamento dei punti facciali. Una volta ricevuta un‚Äôimmagine RGB in input, viene riconosciuto un numero fissato di punti per ogni area del viso in questione.

```sh
preds = fa.get_landmarks(input_img)[-1]
# 2D-Plot
plot_style = dict(marker='o',
markersize=4,
linestyle='-',
lw=2)
pred_type = collections.namedtuple('prediction_type', ['slice', 'color'])
pred_types = {'face': pred_type(slice(0, 17), (0.682, 0.780, 0.909, 0.5)),
'eyebrow1': pred_type(slice(17, 22), (1.0, 0.498, 0.055, 0.4)),
'eyebrow2': pred_type(slice(22, 27), (1.0, 0.498, 0.055, 0.4)),
'nose': pred_type(slice(27, 31), (0.345, 0.239, 0.443, 0.4)),
'nostril': pred_type(slice(31, 36), (0.345, 0.239, 0.443, 0.4)),
'eye1': pred_type(slice(36, 42), (0.596, 0.875, 0.541, 0.3)),
'eye2': pred_type(slice(42, 48), (0.596, 0.875, 0.541, 0.3)),
'lips': pred_type(slice(48, 60), (0.596, 0.875, 0.541, 0.3)),
'teeth': pred_type(slice(60, 68), (0.596, 0.875, 0.541, 0.4))
}
```

Le informazioni ottenute da FAN sono contenute all‚Äôinterno della variabile preds. La variabile **preds** √® un array bidimensionale, caratterizzato dai punti di riferimento facciali individuati dalla rete, in particolare al suo interno sono contenute le coordinate di ogni punto riconosciuto. Dal codice 2.4 possiamo vedere che i punti riconosciuti sono organizzati secondo una struttura ben definita, rappresentata da pred_type. Pred_type √® una struttura che permette di organizzare i punti facciali suddividendoli in 9 aree specifiche del volto, tra cui: faccia (il contorno del viso), sopracciglio sinistro, sopracciglio destro, naso, narici, occhio sinistro, occhio destro, labbra e denti. Ottenuti i punti di riferimento 2D, il compito di FAN √® quello di convertirli in 3D, ovvero creare delle proiezioni a partire dai punti di riferimento facciali 2D. Per realizzare tale obiettivo √® stata introdotta un‚Äôestensione della rete FAN dal 2D al 3D in grado di poter stimare la coordinata z (ovvero la profondit√† di ciascun punto). Una volta ottenute anche tali informazioni, il sistema procede con la realizzazione di grafici 2D e 3D applicati all‚Äôimmagine in input. Di seguito vengono mostrati alcuni esempi di grafici 2D ottenuti dal riconoscimento.

### Rappresentazione del volto riconosciuto

Una volta effettuate le operazioni iniziali di acquisizione e riconoscimento, il sistema prevede la visualizzazione a schermo del risultato ottenuto, caratterizzato da una finestra contenente l‚Äôimmagine 2D presa in input, il grafico 2D presente sul volto riconosciuto e il grafico 3D.

## Manipolazione dei landmarks

Il processo di rilevamento e riconoscimento del volto avviene quindi in due passi principali: per prima cosa, data l‚Äôimmagine acquisita, ne vengono riconosciuti i landmarks tramite l‚Äôutilizzo della rete FAN. Successivamente tali landmarks vengono utilizzati per la realizzazione dei grafici 2D e 3D. Per la creazione di una mappatura 3D del volto √® necessario gestire le coordinate dei landmarks, cio√® convertirle in uno specifico formato per poi essere inviate al server tcp, che ricever√† tali coordinate, le quali verranno decodificate e utilizzate per istanziare i vari landmarks in Unity, ottenendo cos√¨ una mesh 3D del volto riconosciuto.

## Estrazione delle coordinate

La variabile preds √® un‚Äôarray bidimensionale in cui sono contenute le coordinate tridimensionali (x, y, z) di ciascun landmark, per un totale di 68 landmarks. Tale risultato √® ottenuto utilizzando la funzione **get_landmarks(self, image_or_path, detected_faces=None)** contenuta nel file **api.py**, dove al suo interno sono presenti tutte le funzioni necessarie per la rilevazione e il riconoscimento dei landmarks. Tale funzione ha come risultato il valore ottenuto dalla funzione get_landmarks_from_image(image_or_path, detected_faces=None):

```sh
def get_landmarks(self, image_or_path, detected_faces=None): 
  """Deprecated, please use get_landmarks_from_image 
  Arguments: 
    image_or_path {string or numpy.array or torch.tensor} -- The input image or path to it. 
  Keyword Arguments: 
    detected_faces {list of numpy.array} -- list of bounding boxes, one for each face found in the image (default: {None}) """ 
    
    return self.get_landmarks_from_image(image_or_path, detected_faces=None)
```

## Comunicazione server tcp

Per poter mappare il volto in Unity √® necessario l‚Äôutilizzo di un server tcp per la ricezione delle coordinate dei landmarks inviate lato pyton. In Unity il server √® stato sviluppato integrando **slessans**, che consente ai client di connettersi al server e inviare token, cio√® stringhe contenenti le coordinate di ciascun landmark seguite da un separatore ‚Äú&‚Äù. Per la creazione del server √® stato utilizzato un gameObject persistente (sfera) a cui √® stato allegato lo script **SCL_PositionalControllerInput.cs**, che permette di collegarsi ad un client, leggere i 3 valori di posizione (x, y, z) per poi convertirli da formato stringa a float.
L‚Äôinvio delle coordinate al server tcp in Unity √® stato gestito in python mediante l‚Äôutilizzo della libreria socket. Una volta completato l‚Äôapplicativo server lato Unity, si √® proceduto con la creazione del client.

```sh
with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: 
  s.connect(('127.0.0.1', 13000)) 
  i = 0 
  while i < len(preds): 
    string = str(preds[i])+'&'
    msg = bytes((string), 'utf-8')
    s.sendall(msg) 
    time.sleep(0.2) 
    i = i + 1
```

Dal codice precedente, socket.socket √® stato utilizzato per creare un oggetto socket, mentre gli argomenti passati specificano la famiglia di indirizzi, ovvero IPv4, e il tipo di socket. Il client chiama connect(‚Äú127.0.0.1‚Äù,13000) per stabilire una connessione al server attraverso un host e una porta unica. In questo caso solo il client pu√≤ raggiungere il server, e non viceversa. Una volta collegato il client al server, si √® gestito l‚Äôarray preds attraverso l‚Äôuso di una variabile contatore e un ciclo che prendendo in input la lunghezza dell‚Äôarray, viene scorso convertendo le coordinate di ciascun landmark in formato stringa e tramite s.sandall() inviato come oggetto byte al server . Tale operazione si ripete per ogni landmark riconosciuto, con un intervallo di invio impostato a 0.2 secondi. Di seguito √® riportato il risultato ottenuto su console Unity:

![Comunicazione server tcp ](images/coordinate.png)

### Decodifica delle coordinate dei landmark

Per ogni token ricevuto √® opportuno stabilirne un formato specifico, in modo che tali coordinate possano essere utilizzate per istanziare ciascun landmark riconosciuto in Unity. √â possibile vedere che le coordinate che vengono inviate al server presentano un formato caratterizzato dalla presenza di caratteri che non ne permettono l‚Äôutilizzo. Per rimuovere questi caratteri fondamentale √® stato l‚Äôutilizzo del modulo re. Il modulo re fornisce operazioni per la corrispondenza delle espressioni regolari. Tali espressioni regolari usano il carattere (‚Äò\‚Äô) per indicare forme speciali o per consentire l'uso di caratteri speciali senza invocarne il significato, inoltre le funzioni in questo modulo consentono di verificare se una determinata stringa corrisponde a una certa espressione regolare. La soluzione proposta prevede l‚Äôutilizzo di re.sub(), che a seconda del pattern dato in input permette di sostituire i caratteri non richiesti, in questo caso le parentesi quadre. 

Successivamente √® stata implementata la funzione man_string(string), in grado di assegnare a ciascuna stringa un formato valido. La funzione man_string() riceve come parametro la stringa ottenuta da re.sub(), la quale √® concatenata con il carattere ‚Äú&‚Äù affinch√© il server riconosca il token correttamente. Per evitare la presenza di eventuali errori di formato si √® poi aggiunto un ulteriore controllo, cio√® ‚Äústring.strip()‚Äù per verificare se il token presenta spazi iniziali e finali, in tal caso verranno rimossi. Di seguito viene riportato il risultato finale utilizzando la funzione man_string(string):

![Decodifica delle coordinate dei landmark ](images/coordinate-finali.png)

### Rappresentazione Volto in Unity

Una volta estrapolate le coordinate dai landmarks riconosciuti, lo sviluppo del sistema si √® poi concentrato sulla loro rappresentazione grafica in Unity, ottenendo lo stesso grafico generato nella GUI di matplotlib. Si e proceduto utilizzando le coordinate di formato x,y,z di ciascun landmark per istanziare un gameObject di tipo sfera. La sfera, chiamata ‚Äúmarker‚Äù, √® un gameObject di tipo prefab. Per poter istanziare correttamente ogni landmark √® stato utilizzato lo script CreaOggetti.cs. Innanzitutto sono state istanziate la variabile **controllerInput**, che permette di comunicare con SCL_PositionalControllerInput per prendere in input le coordinate ricevute e decodificate, e **proiettile** per istanziare il gameObject sfera. Affinch√® ogni prefab possa essere istanziato √® stato utilizzato il metodo Update(), in quanto essendo richiamato ad ogni frame risulta il pi√π adatto per ricevere in real-time le coordinate. Tale tecnica permette la realizzazione di una singola mappatura 3D del volto riconosciuto.

![Rappresentazione Volto in Unity ](images/volto.png)

### Orientamento del volto

Per riconoscere l‚Äôorientamento del volto √® stato utilizzato lo script rettaPerpendicolare.cs, il quale √® stato aggiunto ad un gameObject con mesh filter, necessario per la generazione di un raggio luminoso indicante la direzione. L‚Äôorientamento del volto √® calcolato prendendo in considerazione le coordinate di tre landmarks specifici del volto: Landmark occhio sinistro, Landmark occhio destro, Landmark labbro superiore.
Una volta salvato il vettore di posizione di ciascun landmark, lo script rettaPerpendicolare.cs richiama tali vettori per istanziare tre gameObject sfera, necessari per la generazione del raggio. Cos√¨ come in CreaOggetti.cs, anche in questo script la procedura viene svolta in Update(), in cui viene effettuato un controllo per verificare se tutti i prefab del volto sono stati istanziati correttamente. A questo punto, per poter generare il raggio √® stata richiamata la variabile mesh, a cui sono stati passati come vertici i vettori di posizione precedenti e utilizzato il metodo RecalculateNormals() per ricalcolare le normali della mesh da triangoli e vertici. Da questo si √® proceduto istanziando la componente Linerender lr, la quale permette di generare una linea retta nello spazio 3D circostante ed √® stato aggiunto un materiale affinch√© sia visibile dalla rappresentazione finale.

La posizione iniziale della linea √® stata calcolata individuando il punto medio tra i due occhi, mentre la posizione finale √® ottenuta sommando alle normali della mesh il vettore di posizione del gameObject in cui lo script √® stato aggiunto. Il risultato finale √® mostrato come segue:

![Orientamento del volto ](images/mask.gif)

### Integrazione in Ambientazioni 3D e Videogames

Determinato l'orientamento del volto, si √® deciso di integrare il sistema in alcuni progetti di tesi sviluppati dai alcuni miei colleghi. Trattandosi di giochi e ambientazioni 3D basate sul riconoscimento dell‚Äôemozione dell‚Äôutente, l‚Äôidea √® stata quella di poter integrare questo progetto per coinvolgere maggiormente l‚Äôutente nella sessione di gioco e nell‚Äôambientazione circostante. Per mettere in pratica tale idea si √® proceduto analizzando i modi e le dinamiche con cui l‚Äôutente possa interagire maggiormente con ogni applicativo sviluppato, rendendolo divertente e avvincente. Mentre l‚Äôutente sta giocando oppure se si trova in una particolare ambientazione, viene rilevato il suo volto, identificato l‚Äôorientamento e da esso generati oggetti che vanno a ostacolare/sorprendere l‚Äôutente. Lo spawn degli oggetti √® stato realizzato in modo che quando l‚Äôutente guarda in una direzione, l‚Äôoggetto viene generato dalla direzione opposta. Analizzando la direzione del volto e in particolare il raggio generato, si √® pensato di dividere l‚Äôarea visiva dell‚Äôutente in tre sezioni: sinistra, centro e destra.
In un videogame incentrato sull‚ÄôAir Hockey, gli oggetti che vengono creati sono pietre, le quali diventano parte attiva del game. Le pietre nel caso in cui l‚Äôutente guarda a sinistra o destra vengono fatte cadere sul tavolo da gioco impedendo il passaggio del dischetto e disturbando l‚Äôutente nel gioco. 

Di seguito viene mostrato il risulto finale:

![Integrazione in Ambientazioni 3D e Videogames ](images/air-hockey.png)

In un altro progetto la tecnica utilizzata √® stata la medesima, ma essendo un applicativo con diverse ambientazioni 3D, la generazione degli oggetti √® risultata singolare per ognuno. Nell‚Äôambientazione spazio, per cercare di rendere l‚Äôutente sorpreso √® stato fatto in modo che gli oggetti creati fossero caratteristici, generando un‚Äôastronave con effetti luminosi nel caso in cui l‚Äôutente guarda a sinistra, e un robottino nel caso in cui l‚Äôutente guarda a destra.

![Integrazione in Ambientazioni 3D e Videogames ](images/spazio.png)

Nell‚Äôambientazione Natura l‚Äôutente si trova in un ambiente tranquillo, caratterizzato da un paesaggio con alberi e colline. Per far s√¨ che l‚Äôutente possa interagire maggiormente, gli oggetti che vengono creati in base all‚Äôorientamento del volto sono animali, in particolare viene generato un coniglio se l‚Äôutente guarda alla sua sinistra e un maialino se l‚Äôutente guarda alla sua destra.

![Integrazione in Ambientazioni 3D e Videogames ](images/natura.png)

Tale sistema, integrandolo in altri progetti in cui si richiedeva un maggior coinvolgimento degli utenti nell'ambientazione, ha quindi evidenziando una certa flessibilit√† di utilizzo e validit√†.

### Ambiente di Sviluppo
